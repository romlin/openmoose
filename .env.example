# ╔══════════════════════════════════════════════════╗
# ║           OpenMoose Configuration                ║
# ║   Copy to .env and fill in your values:          ║
# ║     cp .env.example .env                         ║
# ╚══════════════════════════════════════════════════╝

# ── Gateway ──────────────────────────────────────────
# Port for the HTTP/WebSocket server
GATEWAY_PORT=18789

# ── LLM Provider ────────────────────────────────────
# Which backend to use: 'node-llama-cpp' (integrated local) or 'mistral' (cloud)
LLM_PROVIDER=node-llama-cpp

# ── Local Inference (Integrated) ───────────────────
# Only used when LLM_PROVIDER=node-llama-cpp
# LLM GPU: 'auto', 'metal' (macOS), 'cuda' (NVIDIA), 'vulkan' (Linux/Windows), or 'false' (CPU)
# Set to 'false' if you see "ggml_vulkan: Failed to allocate pinned memory" errors.
LLAMA_CPP_GPU=auto
LLAMA_CPP_MODEL_PATH=models/llama-cpp/ministral-8b-reasoning-q4km.gguf

# ── Mistral AI (Cloud) ─────────────────────────────
# Only used when LLM_PROVIDER=mistral
MISTRAL_MODEL=mistral-large-latest
MISTRAL_API_KEY=

# ── Text-to-Speech ──────────────────────────────────
# Language: en, ko, es, pt, fr
TTS_LANG=en
# Inference steps (lower = faster, 2 is a good default)
TTS_STEPS=4
# Playback speed multiplier (1.0 = normal)
TTS_SPEED=1.05

# ── Memory ──────────────────────────────────────────
# Path to the LanceDB vector database
MEMORY_DB_PATH=.moose/memory

# ── Browser Sandbox ─────────────────────────────────
# Custom directory for browser profiles (optional, defaults to .moose/data/browser-profiles)
# BROWSER_PROFILE_DIR=

# ── Logging ─────────────────────────────────────────
# Level: debug, info, warn, error
LOG_LEVEL=info
# Set to 'true' to suppress all console output
LOG_SILENT=false
